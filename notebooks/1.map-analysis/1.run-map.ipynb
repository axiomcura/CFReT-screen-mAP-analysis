{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Executing Mean Average Precision (mAP)\n",
    "\n",
    "Mean Average Precision (mAP) is a flexible statistical framework used to measure the **phenotypic activity** of compounds by comparing them to control groups. In this notebook, we utilize high-content screening data, that used the CellPainting assay, to identify potential drug candidates that demonstrate evidence of reversing the effects of cardiac fibrosis. The dataset comprises **image-based profiles at the replicate level (well-level)**.\n",
    "\n",
    "#### **Controls Used in the Screen**\n",
    "To interpret mAP scores, we leverage the following control groups:\n",
    "- **Negative control**: Failing CF cells treated with DMSO.\n",
    "- **Positive control**: Healthy CF cells treated with DMSO.\n",
    "\n",
    "#### **Interpreting mAP Scores**\n",
    "- **High mAP Scores**:  \n",
    "  Indicate that wells treated with a specific compound are highly phenotypically distinct compared to the control. This suggests the compound induces a strong and specific phenotypic change.\n",
    "  \n",
    "- **Low mAP Scores**:  \n",
    "  Indicate that wells treated with a specific compound are phenotypically similar to the control. This suggests the compound has little to no phenotypic effect or a nonspecific one.\n",
    "\n",
    "#### **Biological Interpretation**\n",
    "mAP scores help determine which compounds exhibit phenotypic changes that resemble those of healthy cells, making them potential candidates for reversing the effects of cardiac fibrosis. By comparing the phenotypic activity of compounds to both positive and negative controls, we can prioritize compounds for further validation.\n",
    "\n",
    "**what is outputted**\n",
    "- AP scores generated using both the positive and negative controls\n",
    "- mAP scores generated using both the positive and negative controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erikserrano/Programs/miniconda3/envs/cfret-map/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "from pycytominer.cyto_utils import load_profiles\n",
    "from tqdm import TqdmWarning\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils import io_utils, data_utils, analysis_utils\n",
    "\n",
    "# removing warnigns \n",
    "warnings.filterwarnings(\"ignore\", category=TqdmWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up the necessary file paths and directories required for the notebook, ensuring that input files exist. \n",
    "It also creates a results folder if it doesn't already exist to store outputs generated during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the base data directory and ensure it exists (raises an error if it doesn't)\n",
    "main_results_dir = pathlib.Path(\"./results/\").resolve(strict=True)\n",
    "data_dir = pathlib.Path(\"../data/\").resolve(strict=True)\n",
    "agg_data_dir = (data_dir / \"agg_fs_profiles\").resolve(strict=True)\n",
    "fs_profiles_paths = list((data_dir / \"agg_fs_profiles\").resolve(strict=True).glob(\"*.parquet\"))\n",
    "\n",
    "# Setting the metadata directory for updated plate maps and ensure it exists\n",
    "metadata_dir = pathlib.Path(\"../data/metadata/updated_platemaps\").resolve(strict=True)\n",
    "\n",
    "# Path to the updated barcode plate map file, ensure it exists\n",
    "platemap_path = (metadata_dir / \"updated_barcode_platemap.csv\").resolve(strict=True)\n",
    "\n",
    "# Path to the configuration file (does not enforce existence check here)\n",
    "config_path = pathlib.Path(\"../config.yaml\").resolve(strict=True)\n",
    "\n",
    "# Setting the results directory, resolve the full path, and create it if it doesn't already exist\n",
    "map_results_dir = pathlib.Path(\"./results/map_scores\").resolve()\n",
    "map_results_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading config and general configs\n",
    "configs = io_utils.load_config(config_path)\n",
    "general_configs = configs[\"general_configs\"]\n",
    "\n",
    "# loading bar code\n",
    "barcode = pd.read_csv(platemap_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these files have undergone feature selection, it is essential to identify the overlapping feature names to ensure accurate and consistent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of shared columns among all profiles:\n",
      "482\n"
     ]
    }
   ],
   "source": [
    "# finding shared features while deleting duplicate column names\n",
    "shared_cols = data_utils.find_shared_features(profile_paths=fs_profiles_paths, delete_dups=True)\n",
    "\n",
    "# saving shared features to a json file\n",
    "# if the file already exists, it will not be overwritten\n",
    "if not (main_results_dir / \"shared_features.json\").exists():\n",
    "    print(\"shared_features.json does not exist. Saving shared features to a json file.\")\n",
    "    shared_cols_dict = {}\n",
    "    shared_cols_dict[\"shared_features\"] = shared_cols\n",
    "    with open(main_results_dir / \"shared_features.json\", \"w\") as shared_file:\n",
    "        json.dump(shared_cols_dict, shared_file)\n",
    "\n",
    "# if the file already exists, then we check if the shared features are the same\n",
    "else:\n",
    "    with open(main_results_dir / \"shared_features.json\", \"r\") as shared_file:\n",
    "        shared_cols_dict = json.load(shared_file)\n",
    "    assert shared_cols == shared_cols_dict[\"shared_features\"], \"Shared features are not the same\"\n",
    "\n",
    "# total amount of shared columns among all profiles in batch 1\n",
    "print(\"Total amount of shared columns among all profiles:\")\n",
    "print(len(shared_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the code processes and organizes data by grouping related files and enriching them with additional metadata. Each group is assigned a unique identifier, and the corresponding data files are systematically loaded and prepared. New metadata columns are generated by combining existing information to ensure consistency and clarity. Additional metadata is integrated into the data to provide valuable experimental context, while unique identifiers are added to distinguish the aggregated profiles from different batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix for aggregated profiles\n",
    "aggregated_file_suffix = \"aggregated_post_fs.parquet\"\n",
    "\n",
    "# dictionary to store loaded plate data grouped by batch\n",
    "loaded_plate_batches = {}\n",
    "loaded_shuffled_plate_batches = {}\n",
    "\n",
    "# iterate over unique platemap files and their associated plates\n",
    "for batch_index, (platemap_filename, associated_plates_df) in enumerate(\n",
    "    barcode.groupby(\"platemap_file\")\n",
    "):\n",
    "    # generate a unique batch ID\n",
    "    batch_id = f\"batch_{batch_index + 1}\"\n",
    "\n",
    "    # load the platemap CSV file\n",
    "    platemap_path = (metadata_dir / f\"{platemap_filename}.csv\").resolve(strict=True)\n",
    "    platemap_data = pd.read_csv(platemap_path)\n",
    "\n",
    "    # extract all plate names associated with the current platemap\n",
    "    plate_barcodes = associated_plates_df[\"plate_barcode\"].tolist()\n",
    "\n",
    "    # list to store all loaded and processed aggregated plates for the current batch\n",
    "    loaded_aggregated_plates = []\n",
    "    loaded_shuffled_aggregated_plates = []\n",
    "\n",
    "    for plate_barcode in plate_barcodes:\n",
    "        # resolve the file path for the aggregated plate data\n",
    "        plate_file_path = (\n",
    "            agg_data_dir / f\"{plate_barcode}_{aggregated_file_suffix}\"\n",
    "        ).resolve(strict=True)\n",
    "\n",
    "        # load the aggregated profile data for the current plate\n",
    "        aggregated_data = load_profiles(plate_file_path)\n",
    "\n",
    "        # update loaded data frame with only shared features\n",
    "        aggregated_data = aggregated_data[shared_cols]\n",
    "\n",
    "        # add a new column indicating the source plate for each row\n",
    "        aggregated_data.insert(0,\"Metadata_plate_barcode\" , plate_barcode)\n",
    "\n",
    "        # append the processed aggregated data for this plate to the batch list\n",
    "        loaded_aggregated_plates.append(aggregated_data)\n",
    "\n",
    "        # adding shuffled aggregated profiles\n",
    "        shuffled_aggregated_data = data_utils.shuffle_features(aggregated_data)\n",
    "\n",
    "        # append the processed and shuffled aggregated data for this plate to the batch list\n",
    "        loaded_shuffled_aggregated_plates.append(shuffled_aggregated_data)\n",
    "\n",
    "    # combine all processed plates for the current batch into a single DataFrame\n",
    "    combined_aggregated_data = pd.concat(loaded_aggregated_plates)\n",
    "    meta_concat, feats_concat = data_utils.split_meta_and_features(combined_aggregated_data)\n",
    "\n",
    "    # combine all shuffled and processed plates for the current batch into a single DataFrame\n",
    "    shuffled_combined_aggregated_data = pd.concat(loaded_shuffled_aggregated_plates)\n",
    "    meta_concat, feats_concat = data_utils.split_meta_and_features(shuffled_combined_aggregated_data)\n",
    "\n",
    "    # store the combined DataFrame in the loaded_plate_batches dictionary\n",
    "    loaded_plate_batches[batch_id] = combined_aggregated_data\n",
    "    loaded_shuffled_plate_batches[batch_id] = shuffled_combined_aggregated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running mAP only on controls across all plates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we calculate the mAP (mean Average Precision) scores between controls to assess their quality. Specifically, we aim to evaluate how the negative control compares when using a positive control as a reference, and vice versa. This analysis helps determine whether the controls in the experiment are reliable indicators of quality and consistency. Reliable controls are critical for ensuring the validity of the experiment's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erikserrano/Development/CFReT-screen-mAP-analysis/utils/analysis_utils.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dmso_profile[\"Metadata_treatment_type\"] = \"control\"  # Tag all rows as control\n",
      "/home/erikserrano/Development/CFReT-screen-mAP-analysis/utils/analysis_utils.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dmso_profile[\"Metadata_control_type\"] = dmso_profile.apply(\n",
      "/home/erikserrano/Development/CFReT-screen-mAP-analysis/utils/analysis_utils.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dmso_profile = dmso_profile.reset_index().rename(\n",
      "                                     \r"
     ]
    }
   ],
   "source": [
    "# calculating mAP scores only on with original DMSO profiles \n",
    "analysis_utils.calculate_dmso_map_batch_profiles(\n",
    "    batched_profiles=loaded_plate_batches,\n",
    "    configs=configs,\n",
    "    outdir_path=map_results_dir,\n",
    "    shuffled=False,\n",
    ")\n",
    "\n",
    "# calculating mAP scores only on with shuffled DMSO profiles \n",
    "analysis_utils.calculate_dmso_map_batch_profiles(\n",
    "    batched_profiles=loaded_shuffled_plate_batches,\n",
    "    configs=configs,\n",
    "    outdir_path=map_results_dir,\n",
    "    shuffled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating mAP scores on only treatments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we analyze a high-content screening dataset generated from cell painting experiments, where failing cardiac fibroblasts are treated with multiple compounds. Our goal is to calculate the mean average precision (mAP) by comparing the experimental treatments to two controls: a negative control consisting of DMSO-treated failing cardiac fibroblasts and a positive control consisting of DMSO-treated healthy cardiac fibroblasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "# Here we execute mAP pipeline with with the original \n",
    "analysis_utils.calculate_trt_map_batch_profiles(\n",
    "    batched_profiles=loaded_plate_batches,\n",
    "    configs=configs,\n",
    "    outdir_path=map_results_dir,\n",
    "    shuffled=False\n",
    ")\n",
    "\n",
    "# Here we execute mAP pipeline with with the shuffled dataset \n",
    "analysis_utils.calculate_trt_map_batch_profiles(\n",
    "    batched_profiles=loaded_shuffled_plate_batches,\n",
    "    configs=configs,\n",
    "    outdir_path=map_results_dir,\n",
    "    shuffled=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfret-map",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
